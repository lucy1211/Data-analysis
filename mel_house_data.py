# -*- coding: utf-8 -*-
"""Mel_house data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b_IotRnPiv3Spyp3rsyIXN3W_-vKLvGz
"""

from google.colab import files
house_data = files.upload()

!pip install pygam

## importing packages
from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split,KFold,cross_val_score
from sklearn.pipeline import Pipeline
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso, Ridge,LinearRegression,BayesianRidge
from sklearn.metrics import mean_squared_error as mse
from sklearn.model_selection import GridSearchCV
import numpy as np
from pygam import LinearGAM,s,f,l
from collections import Counter

import pandas as pd
house_data = pd.read_csv('melb_data.csv')
## date is in object type => convert to datetime type
house_data.Date=pd.to_datetime(house_data.Date, format="%d/%m/%Y")

## convert price into thousand dollar
house_data.Price = house_data.Price/1000
house_data.rename(columns={"Price": "price(in thousand)"})
house_data[house_data.SellerG=='Nelson']

"""Some analysis on the dataset"""

rev_seller=house_data.groupby([pd.Grouper(key='Date', freq='M'), 'SellerG']).Price.sum().to_frame().reset_index()
rev_seller['month_year'] = rev_seller['Date'].dt.to_period('M')
rev_seller.groupby('month_year')[['Price','SellerG']].max() ## returing the name of seller having the highest revenue each month

nhouse_seller=house_data.groupby([pd.Grouper(key='Date', freq='M'), 'SellerG']).SellerG.count().to_frame()
nhouse_seller=nhouse_seller.rename(columns={'SellerG': 'count_sales'}).reset_index()

nhouse_seller['month_year'] = nhouse_seller['Date'].dt.to_period('M')
nhouse_seller.groupby('month_year')[['count_sales','SellerG']].max() ## return the name of seller sold the highest no of houses each month

## the house prices is high in the South,Eastern Metropolitian and more cheaper in the West, North and Eastern Victoria area
price_mean= house_data.groupby('Regionname').Price.mean().nsmallest(3)
area_name = house_data.Regionname.unique()
## function 
def best_index(series):
  counter_series = Counter(series)
  counter_series= sorted(counter_series.items(), key= lambda v:v[1], reverse=True)
  return counter_series[0][0], counter_series[0][1]


best_loc, number_loc = best_index(house_data.Regionname)
best_seller, no_house = best_index(house_data.SellerG)

print('Pp prefer house located in {0}, with total no of houses sold in this area is {1}'.format( best_loc,number_loc))
print('the best real estate seller is {0}, sold {1} houses in total'.format(best_seller, no_house))

## Delve into nelson's sales
regionname_nelson = house_data[house_data.SellerG=='Nelson'].Regionname
region_nelson=pd.DataFrame.from_dict(Counter(regionname_nelson), orient='index').reset_index()
region_nelson.columns=['regions', 'no_houses']
region_nelson['no_houses']= round(region_nelson['no_houses']/no_house*100,3)

for area in area_name:
  if area in regionname_nelson.unique():
    percent = region_nelson[region_nelson.regions==area].no_houses.values[0]
    print('{0}% of total houses locate in {1}'.format(percent,area))
  else:
    print('Nelson did not sell any house in {}'.format(area))

## house price in each area and according to house type
x=house_data.groupby(['Regionname', 'Type']).Price.agg(np.mean)
x=x.to_frame()

## price/region/month 
## visualize

price_month = house_data.groupby([pd.Grouper(key='Date', freq='M'), 'Regionname']).Price.mean().to_frame().reset_index(level=[0,1])
price_month['month_year']= price_month['Date'].dt.to_period('M')
price_pivot = price_month.pivot(index='month_year', columns='Regionname',values='Price')
ax=price_pivot.plot(kind='line', marker='o', figsize=(20,8))
ax.legend(ncol=2)
ax.set_ylabel('avg. price monthly')

## On average, the house prices in metro is more expensive compared to in the victorian area
## in metro area, the house prices in the north and west are not far different, while since jul-2016, the house price in south-eastern significantly increased
## and became one of 3 areas in metro where the house price is costly after eastern and southern.
## price of houses in western victoria is cheapest in all regions on average

for area in area_name:

  x_plus=x.loc[area].reset_index()
  htype_max=x_plus[x_plus['Price']==x_plus.Price.max()].Type.values[0]
  price_max = round(x_plus.Price.max(),3)

  htype_min=x_plus[x_plus['Price']==x_plus.Price.min()].Type.values[0]
  price_min = round(x_plus.Price.min(),3)

  print('In the {0}, the {1} house type is the most expensive with avg. price is {2}, whereas {3} is the cheapest with avg. price of {4}\n '.format(area,htype_max,price_max, htype_min,price_min))

plt.subplots(figsize=(20,8))
sns.boxplot(x='Regionname',y='Price', hue='Type', data=house_data)
## outliers in price variables
## price of h house type is significantly different across regions, highest in the Southern metropolian
## while the price of u-house is seemingly pretty stable, indifferent across regions 
## In Metroplolian area, there are more options in terms of house type relative to the victoria area

type_reg = house_data.groupby(['Regionname', 'Type']).Type.count()
x=type_reg.to_frame()

x.columns=['count_type']
x=x.reset_index(level=[0,1])
pivot_x=x.pivot(index='Regionname', columns='Type', values='count_type')

## bar plot- number of each type houses sold in each region in Mel
ax=pivot_x.plot.bar(figsize=(20,8),rot=0)
ax.set_ylabel('sales')
plt.show()
### the h house type is prominent over other types in all regions 
### the second most popular house type is u but its popularity focuses on the metropolitian areas

### analysing using date variable
house_date=house_data.groupby([pd.Grouper(key='Date', axis=0, freq='M'),'Regionname']).SellerG.count()
house_date=house_date.to_frame().reset_index(level=[0,1])
house_date['month_year']=house_date['Date'].dt.to_period('M')

pivot_hd = house_date.pivot(index='month_year', columns='Regionname', values='SellerG')
pivot_hd=pivot_hd.reset_index()

sales= house_data.groupby(pd.Grouper(key='Date', axis=0, freq='M')).SellerG.count().to_frame()
sales['month_year']=sales.index.to_period('M')

fig,axes = plt.subplots(1,2,figsize=(20,5))
color= 'tab:green'

## bar plot
axes[0].set_title('Sales')
pivot_hd.plot(x='month_year',kind='area', ax= axes[0])
axes[0].tick_params(axis='y')
axes[0].legend(ncol=2)

##line plot
color='tab:red'
sales.plot(x='month_year',marker='o', ax=axes[1], color='blue')
axes[1].set_title('Total sales each month')

plt.show()

### Southern and northern metro are 2 areas where pp are willing to buy houses, best-selling house type is h 
# in 2016, in general, the house sales raised before drastically dropped in Jan-2017, then gradually increased during the rest of this year
## the increase partly stemmed from the rise in sales in the south-eastern metro and eastern areas.



"""Price exploration: 
procedure: separating numerical vs cat variables

=> encoding cat var
=> splitting
=> standardize => select features => fitting...

---

Linear regressions with one-hot encoding for categorical variables
"""

## number of NA values
NA_no = {col:sum(house_data[col].isnull()) for col in house_data.columns}
data = house_data.dropna(axis=0, subset=['Car'])
cols_no_missing = [col for col in data.columns if data[col].isnull().any()== False]
data_test = data[cols_no_missing]

## cat and numerical variables

categorical_cols = [col for col in data_test.columns if data_test[col].dtype=='object']
numerical_cols = [col for col in data_test.columns if data_test[col].dtype!='object']

## variables selection - num: based on correlation / cat: less than 10 categories
numerical_features = [col for col in numerical_cols if abs(data_test[col].corr(data_test.Price))>0.2]
cat_features =[col for col in categorical_cols if data_test[col].nunique() <10]
## get rid of long an lat-titude in numerical features
numerical_features = numerical_features[:-2]
test_col = numerical_features + cat_features

## visualize
sns.set_style('whitegrid')
plt.figure(figsize=(20,90))

for i in range(len(test_col)):
  plt.subplot(12,3,i+1)
  sns.scatterplot(x=data_test[test_col[i]], y= data_test.Price)

##### one-hot encoding since categorical variables are nominal variables
one_enc_df = pd.get_dummies(data_test[test_col], prefix='_')
X_one_enc = one_enc_df.drop('Price', axis =1)
y_one_enc = one_enc_df.Price
sc_mm = ('minmaxscaler',MinMaxScaler())
sc_ss= ('stdscaler',StandardScaler())
oh_mm =[]
oh_ss=[]

oh_mm.append(('LinearRegression',Pipeline([sc_mm,('LinearRegression', LinearRegression())])))
oh_mm.append(('Ridge', Pipeline([sc_mm,("Ridge", Ridge())])))
oh_mm.append(('Lasso', Pipeline([sc_mm,("Lasso", Lasso())])))
oh_mm.append(('BayesianRidge', Pipeline([sc_mm,("BayesianRidge", BayesianRidge())])))

oh_ss.append(('LinearRegression',Pipeline([sc_ss,('LinearRegression', LinearRegression())])))
oh_ss.append(('Ridge', Pipeline([sc_ss,("Ridge", Ridge())])))
oh_ss.append(('Lasso', Pipeline([sc_ss,("Lasso", Lasso())])))
oh_ss.append(('BayesianRidge', Pipeline([sc_ss,("BayesianRidge", BayesianRidge())])))
#### Kfold CV -> grid search
seed =10
split = 10
model_score_ohmm={}
model_score_ohss={}

for i in oh_mm:
  kf = KFold(n_splits=split, random_state=seed, shuffle=True)
  cv_results = cross_val_score(i[1],X_one_enc,y_one_enc,cv=kf, n_jobs=4, scoring='r2' )
  model_score_ohmm.update({i[0]:cv_results.mean()})

print(sorted(model_score_ohmm.items(), key=lambda v:v[1],reverse=True))

for i in oh_ss:
  kf = KFold(n_splits=split, random_state=seed, shuffle=True)
  cv_results = cross_val_score(i[1],X_one_enc,y_one_enc,cv=kf, n_jobs=4 , scoring='r2')
  model_score_ohss.update({i[0]:cv_results.mean()})

print(sorted(model_score_ohss.items(), key=lambda v:v[1],reverse=True))

### find out in testing linear model the  Ridge with min_max_scaler performs best => grid search for  ridge
grid_para = {}
grid_para['alpha']= list(np.concatenate((np.arange(.1,2,0.1), np.arange(2,5,0.5), np.arange(5,10,1)), axis=0))
#grid_para['estimator__alpha_2'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]
#grid_para['estimator__lambda_1'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
#grid_para['estimator__lambda_2'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]

## minmaxscale0
kf = KFold(n_splits=split, random_state=seed, shuffle=True)
X_one_enc_plus= MinMaxScaler().fit_transform(X_one_enc)
y_one_enc_plus = MinMaxScaler().fit_transform(pd.DataFrame(y_one_enc))
gs = GridSearchCV(Ridge(), grid_para, scoring='r2',n_jobs= 4, cv=kf)
grid_results=gs.fit(X_one_enc_plus,y_one_enc_plus)

ridge_score = cross_val_score(Ridge(alpha=grid_results.best_params_['alpha']), X_one_enc_plus,y_one_enc_plus, cv=kf,
                              scoring='r2', n_jobs=4)
print('Ridge score with best alpha is {:7f}'.format(ridge_score.mean()))

"""Trying with GAM

"""

## label encoding
label_encode = LabelEncoder()
data_label= data_test[test_col].copy()
for i in cat_features:
  data_label[i]=label_encode.fit_transform(data_label[i])


X_data_label= data_label.drop('Price', axis=1)
y_data_label = data_label.Price
X_data_label.describe()
cols_label = X_data_label.columns
cols_label

plt.figure(figsize=(20,70))

for i in range(0,len(data_label.columns)-1):
  plt.subplot(12,3,i+1)
  sns.scatterplot(x=data_test[cols_label[i]], y= y_data_label)

### GAM

gam = LinearGAM(s(0)+s(1)+s(2)+s(3)+f(4)+f(5)+f(6))
gam.fit(X_data_label.values,y_data_label.values)

list(enumerate(gam.terms))

plt.rcParams['figure.figsize'] = (28, 8)
fig,axs = plt.subplots(1,len(data_label.columns)-1);
for i, ax in enumerate(axs):
    XX = gam.generate_X_grid(term=i)
    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))
    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')
    if i == 0:
        ax.set_ylim(-30,30)
    ax.set_title(cols_label[i])

plt.figure()
XX=gam.generate_X_grid(term=2)
plt.scatter(X_data_label['Bathroom'], y_data_label)
plt.plot(XX, gam.predict(XX))




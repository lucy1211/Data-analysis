# -*- coding: utf-8 -*-
"""Mel_house data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b_IotRnPiv3Spyp3rsyIXN3W_-vKLvGz
"""

from google.colab import files
house_data = files.upload()

## importing packages
from sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder
from sklearn.model_selection import train_test_split,KFold,cross_val_score
from sklearn.pipeline import Pipeline
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.linear_model import Lasso, Ridge,LinearRegression,BayesianRidge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error as mse
from sklearn.model_selection import GridSearchCV
import numpy as np
from collections import Counter
from sklearn.preprocessing import PowerTransformer
import plotly.express as px

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
plt.close('all')

import pandas as pd
house_data = pd.read_csv('melb_data.csv')
## date is in object type => convert to datetime type
house_data.Date=pd.to_datetime(house_data.Date, format="%d/%m/%Y")

## convert price into thousand dollar
house_data.Price = house_data.Price/1000
house_data.rename(columns={"Price": "price(in thousand)"})
#house_data[house_data.SellerG=='Nelson']

house_data.head(n=3)

"""Some analysis on the dataset"""

rev_seller=house_data.groupby([pd.Grouper(key='Date', freq='M'), 'SellerG']).Price.sum().to_frame().reset_index()
rev_seller['month_year'] = rev_seller['Date'].dt.to_period('M')
rev_seller.groupby('month_year')[['Price','SellerG']].max() ## returing the name of seller having the highest revenue each month

nhouse_seller=house_data.groupby([pd.Grouper(key='Date', freq='M'), 'SellerG']).SellerG.count().to_frame()
nhouse_seller=nhouse_seller.rename(columns={'SellerG': 'count_sales'}).reset_index()

nhouse_seller['month_year'] = nhouse_seller['Date'].dt.to_period('M')
nhouse_seller.groupby('month_year')[['count_sales','SellerG']].max() ## return the name of seller sold the highest no of houses each month

## the house prices is high in the South,Eastern Metropolitian and more cheaper in the West, North and Eastern Victoria area
price_mean= house_data.groupby('Regionname').Price.mean().nsmallest(3)
area_name = house_data.Regionname.unique()
## function 
def best_index(series):
  counter_series = Counter(series)
  counter_series= sorted(counter_series.items(), key= lambda v:v[1], reverse=True)
  return counter_series[0][0], counter_series[0][1]


best_loc, number_loc = best_index(house_data.Regionname)
best_seller, no_house = best_index(house_data.SellerG)

print('Pp prefer house located in {0}, with total no of houses sold in this area is {1}'.format( best_loc,number_loc))
print('the best real estate seller is {0}, sold {1} houses in total'.format(best_seller, no_house))

## Delve into nelson's sales
regionname_nelson = house_data[house_data.SellerG=='Nelson'].Regionname
region_nelson=pd.DataFrame.from_dict(Counter(regionname_nelson), orient='index').reset_index()
region_nelson.columns=['regions', 'no_houses']
region_nelson['no_houses']= round(region_nelson['no_houses']/no_house*100,3)

for area in area_name:
  if area in regionname_nelson.unique():
    percent = region_nelson[region_nelson.regions==area].no_houses.values[0]
    print('{0}% of total houses locate in {1}'.format(percent,area))
  else:
    print('Nelson did not sell any house in {}'.format(area))

## house price in each area and according to house type
x=house_data.groupby(['Regionname', 'Type']).Price.agg(np.mean)
x=x.to_frame()

## price/region/month 
## visualize

price_month = house_data.groupby([pd.Grouper(key='Date', freq='M'), 'Regionname']).Price.mean().to_frame().reset_index(level=[0,1])
price_month['month_year']= price_month['Date'].dt.to_period('M')
price_pivot = price_month.pivot(index='month_year', columns='Regionname',values='Price')
ax=price_pivot.plot(kind='line', marker='o', figsize=(20,8))
ax.legend(ncol=2)
ax.set_ylabel('avg. price monthly')

## On average, the house prices in metro is more expensive compared to in the victorian area
## in metro area, the house prices in the north and west are not far different, while since jul-2016, the house price in south-eastern significantly increased
## and became one of 3 areas in metro where the house price is costly after eastern and southern.
## price of houses in western victoria is cheapest in all regions on average

for area in area_name:

  x_plus=x.loc[area].reset_index()
  htype_max=x_plus[x_plus['Price']==x_plus.Price.max()].Type.values[0]
  price_max = round(x_plus.Price.max(),3)

  htype_min=x_plus[x_plus['Price']==x_plus.Price.min()].Type.values[0]
  price_min = round(x_plus.Price.min(),3)

  print('In the {0}, the {1} house type is the most expensive with avg. price is {2}, whereas {3} is the cheapest with avg. price of {4}\n '.format(area,htype_max,price_max, htype_min,price_min))

type_reg = house_data.groupby(['Regionname', 'Type']).Type.count()
x=type_reg.to_frame()

x.columns=['count_type']
x=x.reset_index(level=[0,1])
pivot_x=x.pivot(index='Regionname', columns='Type', values='count_type')

## bar plot- number of each type houses sold in each region in Mel with static graph
ax=pivot_x.plot.bar(figsize=(20,8),rot=0)

ax.set_ylabel('sales')

plt.show()



### the h house type is prominent over other types in all regions 
### the second most popular house type is u but its popularity focuses on the metropolitian areas

### plotting the above graph by plotly to be an interactive one
import plotly.graph_objects as go
type_house = house_data.Type.unique()
fig = go.Figure(data=[
  go.Bar(name= type_house[0], x=area_name, y=pivot_x[type_house[0]] ),
  go.Bar(name= type_house[1], x=area_name, y=pivot_x[type_house[1]] ),
  go.Bar(name= type_house[2], x=area_name, y=pivot_x[type_house[2]] )
])
fig.update_layout(title='Each house type sales in each area',
  yaxis = dict(title='Sales', tickfont_size = 14),
  barmode='group',
  bargap = 0.15)
fig.show()

### analysing using date variable
house_date=house_data.groupby([pd.Grouper(key='Date', axis=0, freq='M'),'Regionname']).SellerG.count()
house_date=house_date.to_frame().reset_index(level=[0,1])
house_date['month_year']=house_date['Date'].dt.to_period('M')

pivot_hd = house_date.pivot(index='month_year', columns='Regionname', values='SellerG')
pivot_hd=pivot_hd.reset_index()

sales= house_data.groupby(pd.Grouper(key='Date', axis=0, freq='M')).SellerG.count().to_frame()
sales['month_year']=sales.index.to_period('M')

fig,axes = plt.subplots(1,2,figsize=(20,5))
color= 'tab:green'

## bar plot
axes[0].set_title('Sales')
pivot_hd.plot(x='month_year',kind='area', ax= axes[0])
axes[0].tick_params(axis='y')
axes[0].legend(ncol=2)

##line plot
color='tab:red'
sales.plot(x='month_year',marker='o', ax=axes[1], color='blue')
axes[1].set_title('Total sales each month')

plt.show()

### Southern and northern metro are 2 areas where pp are willing to buy houses, best-selling house type is h 
# in 2016, in general, the house sales raised before drastically dropped in Jan-2017, then gradually increased during the rest of this year
## the increase partly stemmed from the rise in sales in the south-eastern metro and eastern areas.

pop_method, no_times = best_index(house_data.Method)
house_data.groupby(['Regionname', 'Method']).Price.mean()

""""Outliers"

"""

plt.subplots(figsize=(20,8))
sns.boxplot(x='Regionname',y='Price', hue='Type', data=house_data)
plt.show()
## wide price range
## price of h house type is significantly different across regions, highest in the Southern metropolian
## while the price of u-house is seemingly pretty stable, indifferent across regions 
## In Metroplolian area, there are more options in terms of house type relative to the victoria area

fig = px.box(house_data, x= "Regionname", y='Price', color='Type')
fig.show()

## detecting "outliers" in price using IQR with conventional cut-off =1.5

def cal_iqr(data, col, cut_off=1.5):
  q25,q75 = np.percentile(data[col], 25), np.percentile(data[col], 75)
  iqr_val = q75-q25 
  lower_bound, upper_bound = q25 - cut_off * iqr_val, q75 + cut_off * iqr_val 
  above_upper = data[data[col]>upper_bound].shape[0]
  low_lower = data[data[col] < lower_bound].shape[0]

  return lower_bound, upper_bound, above_upper+low_lower

def get_outlier(df,num_cols, cutoff):
  summary_out = []
  col_name= ['Feature', 'no_outliers','Lower bound', 'Upper_bound']
  for col in num_cols:
    lower, upper, total = cal_iqr(df,col, cut_off=cutoff )
    summary_out.append((col,total, lower, upper))
  out_df = pd.DataFrame(summary_out, columns=col_name)
  return out_df


### new house_data after removing all outliers of price
lower_bound, upper_bound,total= cal_iqr(house_data,'Landsize')  
new_data = house_data.copy()
new_data.Price = np.where(new_data.Price >upper_bound, upper_bound,new_data.Price)
new_data.Price = np.where(new_data.Price < lower_bound, lower_bound,new_data.Price)
outlier_df = house_data.loc[(house_data.Price > lower_bound) & (house_data.Price < upper_bound)]

get_outlier(house_data,numerical_cols, 1.5)

"""Price exploration: 
procedure: separating numerical vs cat variables

=> encoding cat var
=> splitting
=> standardize => select features => fitting...

---

Separate numerical & categorical variables
"""

## number of NA values


NA_no = {col:sum(house_data[col].isnull()) for col in house_data.columns}
data = house_data.dropna(axis=0, subset=['Car'])
cols_no_missing = [col for col in data.columns if data[col].isnull().any()== False]
data_test = data[cols_no_missing]
X_data = data_test.drop('Price', axis=1)
y_data = data_test.Price
## cat and numerical variables

categorical_cols = [col for col in X_data.columns if X_data[col].dtype=='object']
numerical_cols = [col for col in X_data.columns if (X_data[col].dtype=='float64') or (X_data[col].dtype=='int64') ]

## variables selection - num: based on correlation / cat: less than 10 categories
#numerical_features = [col for col in numerical_cols if abs(data_test[col].corr(data_test.Price))>0.2]
#cat_features =[col for col in categorical_cols if data_test[col].nunique() <10]
## get rid of long an lat-titude in numerical features
#numerical_features = numerical_features[:-2]
#test_col = numerical_features + cat_features

## visualize
#sns.set_style('whitegrid')
#plt.figure(figsize=(20,90))

#for i in range(len(test_col)):
  #plt.subplot(12,3,i+1)
  #sns.scatterplot(x=data_test[test_col[i]], y= data_test.Price)

"""Data Preprocessing"""

## data transforming
### changing the distribution of numerical features to Gaussian
scaler_ = PowerTransformer(method='yeo-johnson')
trans_df = X_data.copy()
for col in numerical_cols:
  t = scaler_.fit_transform(np.array(X_data[col]).reshape(-1,1))
  trans_df[col]=t.reshape(-1)


plt.figure(figsize=(20,70))

for i in range(0,len(numerical_cols)):
  plt.subplot(12,3,i+1)
  sns.scatterplot(x=trans_df[numerical_cols[i]], y= y_data)

## choosing best features
from sklearn.feature_selection import SelectKBest,f_regression,f_classif
#### select the 5 best numerical features
num_fs = SelectKBest(score_func = f_regression, k=5)
num_fs.fit(trans_df[numerical_cols], y_data)

best_num_cols= num_fs.get_support(indices = True)
num_df = trans_df[numerical_cols].iloc[:,best_num_cols]

## select cat. variables less than 10 categories

cat_fs =[col for col in categorical_cols if X_data[col].nunique() <10]

best_fs = num_df.columns.to_list() + cat_fs 

best_df_old = trans_df[best_fs]
best_df = pd.get_dummies(best_df_old, prefix= '_')

"""Model training"""

sc_ss= ('stdscaler',StandardScaler())
oh_ss=[]



oh_ss.append(('LinearRegression',Pipeline([sc_ss,('LinearRegression', LinearRegression())])))
oh_ss.append(('Ridge', Pipeline([sc_ss,("Ridge", Ridge())])))
oh_ss.append(('Lasso', Pipeline([sc_ss,("Lasso", Lasso())])))
oh_ss.append(('BayesianRidge', Pipeline([sc_ss,("BayesianRidge", BayesianRidge())])))
oh_ss.append(('RandomForestRegressor', Pipeline([sc_ss,("RandomForestRegressor", RandomForestRegressor())])))
oh_ss.append(('GradientBoostingRegressor', Pipeline([sc_ss,("GradientBoostingRegressor", GradientBoostingRegressor())])))
oh_ss.append(('KNeighborsRegressor', Pipeline([sc_ss,("KNeighborsRegressor", KNeighborsRegressor())])))
oh_ss.append(('DecisionTreeRegressor', Pipeline([sc_ss,("DecisionTreeRegressor", DecisionTreeRegressor())])))
oh_ss.append(('XGBRegressor', Pipeline([sc_ss,("XGBRegressor", XGBRegressor(early_stopping_rounds=10,num_boost_round = 100))])))

#### Kfold CV -> grid search
seed =10
split = 10

model_score_ohss={}


for i in oh_ss:
  kf = KFold(n_splits=split, random_state=seed, shuffle=True)
  cv_results = cross_val_score(i[1],best_df,y_data,cv=kf, n_jobs=4 )
  model_score_ohss.update({i[0]:cv_results.mean()})

print(sorted(model_score_ohss.items(), key=lambda v:v[1],reverse=True))

## hyper-parameter tuning
## tuning : 
ss = ('Scaler', StandardScaler())

est= Pipeline([sc_ss,("XGB", XGBRegressor(early_stopping_rounds=10,num_boost_round = 100))])
best =[]
params = {
    'XGB__max_depth': [7,8,10],
    'XGB__eta': [0.001, 0.05, 0.01],
    
}


#kf = KFold(n_splits=split, random_state=seed, shuffle=True)
gs = GridSearchCV(estimator= est,param_grid=params, cv=5, n_jobs=-1)
gs_results =gs.fit(best_df, y_data)

## model is improved with max_depth =7, learning_rate = 0.001 and other parameters are kept as defaults

## refit
new_pipeline = Pipeline([ss,('XGB', XGBRegressor(early_stopping_rounds=10,num_boost_round = 100, max_depth=7, learning_rate=0.001))])
new_pipeline.fit(best_df, y_data)

### find out in testing linear model the  Ridge with min_max_scaler performs best => grid search for  ridge
#grid_para = {}
#grid_para['alpha']= list(np.concatenate((np.arange(.1,2,0.1), np.arange(2,5,0.5), np.arange(5,10,1)), axis=0))
#grid_para['estimator__alpha_2'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]
#grid_para['estimator__lambda_1'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
#grid_para['estimator__lambda_2'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]

## minmaxscale0
#kf = KFold(n_splits=split, random_state=seed, shuffle=True)
#X_one_enc_plus= MinMaxScaler().fit_transform(X_one_enc)
#y_one_enc_plus = MinMaxScaler().fit_transform(pd.DataFrame(y_one_enc))
#gs = GridSearchCV(Ridge(), grid_para, scoring='r2',n_jobs= 4, cv=kf)
#grid_results=gs.fit(X_one_enc_plus,y_one_enc_plus)

#ridge_score = cross_val_score(Ridge(alpha=grid_results.best_params_['alpha']), X_one_enc_plus,y_one_enc_plus, cv=kf,
                              scoring='r2', n_jobs=4)
#print('Ridge score with best alpha is {:7f}'.format(ridge_score.mean()))

